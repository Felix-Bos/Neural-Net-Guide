# Neural-Net-Guide — Building a Deep Learning Library from Scratch

This project is a hands-on notebook-based course where I explain and implement the core principles behind neural networks — both mathematically and programmatically. The objective is to build a mini deep-learning framework from scratch using only NumPy, following the same logic as TensorFlow (forward pass, backpropagation, loss computation, and gradient-based learning).

---

## Overview

The notebook covers step-by-step:

- Forward and backward propagation
- Gradient descent and optimization principles
- Loss functions for regression and classification
- Activation functions and their derivatives
- Network architecture and parameter initialization
- Numerical stability considerations

Each concept is explained theoretically, then implemented in code for full transparency and understanding.

---

## Features

- Custom deep learning mini-framework entirely in NumPy
- Modular neural network layers and training pipeline
- Training loop similar to TensorFlow/Keras
- Support for multiple loss functions and metrics
- Benchmark comparison against TensorFlow on various datasets

---

## Datasets

The framework is tested on multiple classification datasets to compare performance and behavior with TensorFlow.

---

